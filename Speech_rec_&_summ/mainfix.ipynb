{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa3f360-323a-4525-9133-b061fe454088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\gunak\\anaconda3\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: pydub in c:\\users\\gunak\\anaconda3\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from SpeechRecognition) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2022.9.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\users\\gunak\\anaconda3\\lib\\site-packages (3.7)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gunak\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\gunak\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install SpeechRecognition pydub\n",
    "%pip install nltk\n",
    "%pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca7f93-d8d6-4dd2-b959-4d7a193af6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries for convertion\n",
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "\n",
    "# create a speech recognition object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# a function that splits the audio file into chunks\n",
    "# and applies speech recognition\n",
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 300,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=300,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "    # return the text for all chunks detected\n",
    "    return whole_text\n",
    "\n",
    "\n",
    "\n",
    "path = \"JcxVoV1ah78_160.wav\"\n",
    "# print(\"\\nFull text:\", get_large_audio_transcription(path))\n",
    "\n",
    "\n",
    "\n",
    "with open(\"text1.txt\",'w') as f:\n",
    "  f.write(get_large_audio_transcription(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed8be0b-7b9b-4646-9062-1ba8bb13d7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         OVER         \n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries for summary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Read the text file\n",
    "text = open('text1.txt', 'r').read()\n",
    "\n",
    "# Tokenize the text\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Create the K-means clustering model\n",
    "model = KMeans(n_clusters=2)\n",
    "model.fit(X)\n",
    "\n",
    "# Generate the cluster labels\n",
    "labels = model.labels_\n",
    "\n",
    "# Create the summary by selecting the sentences with the highest scores\n",
    "summary = \"\"\n",
    "for j in range(len(sentences)):\n",
    " if labels[j] == 1:\n",
    "   summary += sentences[j]\n",
    "\n",
    "# Print the summary\n",
    "# print(summary)\n",
    "\n",
    "\n",
    "with open(\"text2.txt\",'w') as g:\n",
    "  g.write(summary)\n",
    "\n",
    "\n",
    "\n",
    "print(\"         OVER         \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b60ece8-f367-4fdf-b2c9-7660e2df937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words before summarization: 72\n",
      "Total number of words after summarization: 27\n",
      "Accuracy of summarization is= 62.5\n"
     ]
    }
   ],
   "source": [
    "# Words counter\n",
    "def count_words(filename):\n",
    "    total_words = 0\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "            total_words += len(words)\n",
    "    return total_words\n",
    "\n",
    "filename = \"text1.txt\"\n",
    "a = count_words(filename)\n",
    "print(\"Total number of words before summarization:\", a)\n",
    "\n",
    "\n",
    "filename = \"text2.txt\"\n",
    "b = count_words(filename)\n",
    "print(\"Total number of words after summarization:\", b)\n",
    "\n",
    "\n",
    "#Accuracy percentage\n",
    "percentage_decrease = ((a - b) /a) * 100\n",
    "print(\"Accuracy of summarization is=\",percentage_decrease)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01d685d4-11e4-44f1-8ff9-a5152e157a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count of file berore summarisation:\n",
      " {'Funny': 1, 'saying': 1, 'about': 1, 'the': 5, 'big': 1, 'economic': 2, 'news': 3, 'of': 2, 'day.': 1, 'Actual': 1, 'in': 2, 'any': 1, 'interest': 1, 'rate': 1, 'increase.': 1, 'Hey': 1, 'you': 1, 'know': 1, 'who': 1, 'is.': 1, 'So': 1, 'on': 2, 'this': 2, 'friday': 1, 'program': 1, 'something': 1, 'all': 1, 'it': 1, 'different': 1, 'his': 1, 'own': 1, 'words': 2, 'most': 1, 'used': 1, 'from.': 1, 'Wood': 1, 'number': 1, '1': 1, 'course': 1, \"it's\": 1, 'a': 1, 'big.': 1, 'Big': 1, 'worry': 1, 'pink': 1, 'keeping': 1, 'your': 1, 'mopid': 1, 'night': 1, 'price': 1, 'is': 1, 'whole': 1, 'ball': 1, 'game': 1, 'right': 1, 'now': 1, 'basically': 1, 'said': 1, 'as': 1, 'much.': 1}\n",
      "\n",
      "\n",
      "\n",
      "Word count of file after summarisation:\n",
      " {'So': 1, 'on': 2, 'this': 2, 'friday': 1, 'program': 1, 'something': 1, 'all': 1, 'of': 1, 'it': 1, 'different': 1, 'in': 1, 'his': 1, 'own': 1, 'words': 2, 'most': 1, 'used': 1, 'economic': 1, 'from.Wood': 1, 'number': 1, '1': 1, 'course': 1, \"it's\": 1, 'a': 1, 'big.': 1}\n"
     ]
    }
   ],
   "source": [
    "def count_words(filename):\n",
    "    word_count = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                if word not in word_count:\n",
    "                    word_count[word] = 1\n",
    "                else:\n",
    "                    word_count[word] += 1\n",
    "    return word_count\n",
    "\n",
    "\n",
    "word_count = count_words(\"text1.txt\")\n",
    "print(\"Word count of file berore summarisation:\\n\", word_count)\n",
    "\n",
    "\n",
    "word_count = count_words(\"text2.txt\")\n",
    "print(\"\\n\\n\\nWord count of file after summarisation:\\n\", word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15145c3d-8cad-47ca-99d5-c2b173392f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
